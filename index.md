![alt text](https://i.imgur.com/lQJjpAk.gif)


This thread concerns attempts to construct artificial general intelligence, which I often underline [may likely be mankind's last invention.](https://www.youtube.com/watch?v=9snY7lhJA4c)

I clearly unravel how I came to invent the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), (a component in another description called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)) in relation to quantum computation.

I am asking anybody that knows [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning to pitch in the discussion below.

Part A - What is the goal?
======
(1) The aim is to build [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).

(2) Machine learning often concerns [constraining algorithms with respect to biological examples](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/).

**(3)** Babies are great  examples of **some non-trivial basis** for [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence); babies are significant examples of biological baseis that are reasonably usable to inspire smart algorithms, especially in the aims of (1), regarding (2).

[Thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) together with the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), espouses the **importance** of [considering biological constraints](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) in the aim of developing  [general machine learning models](https://en.wikipedia.org/wiki/Artificial_general_intelligence), pertinently, where [babies&#39; brains are observed to be pre-equipped with particular &quot;**physics priors**&quot;, constituting specifically, the ability for babies to intuitively know laws of physics, while learning by reinforcement](http://science.sciencemag.org/content/348/6230/91).

Note that the phrasing “intuitively know laws of physics”, should not be confused for nobel laureate or physics undergrad aligned babies that do or understand physics exams, but instead, the aforesaid phrasing simply conveys that babies' brains are pre-baked with ways to naturally exercise physics based expectations w.r.t. interactions with objects in their world, as indicated by [Stahl et al](http://science.sciencemag.org/content/348/6230/91)).

The importance of **recognizing underlying causal physics laws in learning models** (although **not via supermanifolds** , as encoded in [Thought Curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)), has recently been both [demonstrated](https://arxiv.org/abs/1606.05579) and separately [echoed by Deepmind](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/), and of late, distinctly [emphasized](https://arxiv.org/abs/1709.08568) by Yoshua Bengio.




Part B - Babies know physics, plus they learn
======
Back in 2016, I [read somewhere that babies know some physics intuitively](https://www.washingtonpost.com/news/speaking-of-science/wp/2015/04/02/new-study-reveals-the-shockingly-complex-thought-processes-of-infants/?utm_term=.dd0b9545030b). 

Also, it is empirically observable that babies use that intuition to develop abstractions of knowledge, [in a reinforcement learning like manner](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621/).



Part C - Algorithms for reinforcement learning and physics
======
Now, I knew beforehand of two types of major deep learning models, that:

(1) used reinforcement learning. ([Deepmind Atari q](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf))

(2) learn laws of physics. ([Uetorch](https://github.com/facebook/UETorch))

**However**:

(a) Object detectors like [(2)](https://github.com/facebook/UETorch) use something called [pooling](http://iamaaditya.github.io/2016/03/one-by-one-convolution/) to gain translation invariance over objects, so that the model learns regardless of where the object in the image is positioned.

(b) Instead, [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  excludes pooling, because [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  requires translation variance, in order for Q learning to apply on the changing positions of the objects in pixels.


Part D - Problem discussion...
======
As a result I sought a model that could deliver both translation invariance and variance at the same time, and reasonably, **part of the solution** was models that disentangled factors of variation, i.e. [manifold learning frameworks](https://arxiv.org/abs/1611.03383).

I didn't stop my scientific thinking at [manifold learning](http://scikit-learn.org/stable/modules/manifold.html) though.

Given that cognitive science may be used to constrain machine learning models ([similar to how firms like Deepmind often use cognitive science as a boundary on the deep learning models they produce](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) I sought to create a disentanglable model that was as constrained by cognitive science, as far as algebra would permit.



Part E - Problem approach...
======
As a result I created something called the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning). (A part of a system called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)). 

This was due to [evidence of supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134); I compacted machine learning related algebra for disentangling, in the regime of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold). This could be seen as an extension of [manifold learning in artificial intelligence](http://scikit-learn.org/stable/modules/manifold.html).

**Given that the [supermanifold hypothesis](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning) compounds** ϕ(x,![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png))<SUP>T</SUP>w, **here is an annotation of the hypothesis:**


1. [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) entails ϕ(x;![alt text](https://i.imgur.com/PRSAGxn.png))<SUP>T</SUP>w, that denotes the input space x, and learnt representations ![alt text](https://i.imgur.com/PRSAGxn.png).
2. [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) underlines that coordinates or latent spaces in the [manifold](https://en.wikipedia.org/wiki/Manifold) framework, are learnt features/representations, or directions that are sparse configurations of coordinates.
3. [Supermathematics](https://en.wikipedia.org/wiki/Supermathematicsg) entails (x,![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png)), that denotes some x valued coordinate distribution, and by extension, directions that compact coordinates via ![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png)
4.  As such, the aforesaid (x,![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png)), is subject to coordinate transformation.
5. Thereafter 1, 2, 3, 4 and [supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134), within the generalizable nature of [euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), reasonably effectuate ϕ(x,![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png))<SUP>T</SUP>w.


Part F - A probable experiment: A Transverse Field Ising Spin (Super)–Hamiltonian Quantum Computation 
=====
![alt text](https://i.imgur.com/scLHFT7.png)


Part G - Limitations
====

**Although** [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is minor particularly in its simple description (acquiescing [SQCD](https://arxiv.org/abs/1104.1425)) in relation to [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_General_Intelligence), it **crucially** delineates that the math of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold) is reasonably applicable in [Deep Learning](https://en.wikipedia.org/wiki/Deep_Learning), imparting that [cutting edge  Deep Learning work tends to consider boundaries in the biological brain](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) while underscoring that biological brains can be **optimally** evaluated using [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) operations.

In broader words, [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) occurs on the following evidence:

1. [Manifolds](https://en.wikipedia.org/wiki/Manifold) are in the regime of very [general algorithms](http://scikit-learn.org/stable/modules/manifold.html), that enable models to **learn many degrees of freedom** in latent space, (i.e. size, scale etc… where said degrees are observable as features of **physics** interactions) where transformations on points may represent for e.g., features of a particular object in pixel space, and transformations on said points or weights of an object are **disentangleable** or separable from those pertaining to other objects in latent space. ([Early Visual Concept Learner](https://arxiv.org/abs/1606.05579), [Mean field theory expressivity networks](https://arxiv.org/abs/1606.05340), etc)

2. Given (1), and the generalizability of  [euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), together with the instance that there persists  [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) measurements in biological brains, thought curvature predicates that  [Supermathematics](https://en.wikipedia.org/wiki/Supermathematics) or  [Lie Superalgebras](https://en.wikipedia.org/wiki/Lie_Superalgebras) (in  [Supermanifolds](https://en.wikipedia.org/wiki/Supermanifold)) may reasonably, empirically apply in  [Deep Learning](https://en.wikipedia.org/wiki/Deep_Learning), or some other named study of hierarchical learning in research.

Part H - A brief discussion on the significance of a Transverse Field Ising Spin (Super)-Hamiltonian learning algorithm
======
The usage of [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) operations is **imperatively** efficient, as such operations enable **deeply abstract representations** (as is naturally afforded by symmetry group Lie Superalgebras (See [source 1](https://arxiv.org/abs/0705.1134), [source 2](https://en.wikipedia.org/wiki/Symmetry_group#See_also))), pertinently, in a [general, biologically tenable time-space complex optimal regime](https://arxiv.org/abs/0705.1134). 

As such, said **deeply abstract representations** may **reasonably capture** certain **“physics priors”**, (See **page 4** of the [paper](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)) abound the [laws of physics](https://en.wikipedia.org/wiki/Laws_of_science).  


Part I - Considerations
======
Notably, an ***initial degree*** of the
[(Super-) Hamiltonian](https://arxiv.org/abs/hep-th/0506170) structure required by [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) shall require a quite scalable scheme, such as some [boson sampling](https://en.wikipedia.org/wiki/Boson_sampling) aligned range, in conjunction with [supersymmetric space](https://arxiv.org/abs/0705.1134). This scheme is approachable [on the scale of 42 qubits](https://www.researchgate.net/publication/316643055_No_imminent_quantum_supremacy_by_boson_sampling#pf6), or a 42 qubit = ![alt text](https://i.imgur.com/qTHl2uA.png) gb = 131,072 gb ram configuration for simple task/circuit tests.

Separately, I am working to determine how feasible the  [model](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is.

I am working to design suitable experiments, and figuring out what type of ![alt text](https://i.imgur.com/4P5rY64.png) (training samples) are sufficiently applicable to the [model](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis).

Remember, if you have good knowledge of [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning, you may pitch in for a discussion, by messaging me at jordanmicahbennett@gmail.com.

Part J - Extras
======

I compiled a list of resources (beyond things cited throughout the papers) that may be helpful [here](https://github.com/JordanMicahBennett/Supermathematics-and-Artificial-General-Intelligence/blob/master/1.%20Extra%20list%20of%20helpful%20resources.md).



Part K - Article Download
======
Here is this article in [pdf form](https://www.researchgate.net/publication/319523372_Supermathematics_and_Artificial_General_Intelligence).


