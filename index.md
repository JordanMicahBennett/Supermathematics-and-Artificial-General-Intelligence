![alt text](https://i.imgur.com/lQJjpAk.gif)


This thread concerns attempts to construct artificial general intelligence, which I often underline [may likely be mankind's last invention.](https://www.youtube.com/watch?v=9snY7lhJA4c)

I clearly unravel how I came to invent the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), (a component in another description called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)) in relation to quantum computation.

I am asking anybody that knows [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning to pitch in the discussion below.

Part A - What is the goal?
======
(1) The aim is to build [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).

(2) Machine learning often concerns [constraining algorithms with respect to biological examples](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/).

(3) Babies are great  examples of **some non trivial basis** for [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence); babies are significant examples of biological baseis that are reasonably usable to inspire smart algorithms, especially in the aims of (1), regarding (2).



Part B - Babies know physics, plus they learn
======
Back in 2016, I [read somewhere that babies know some physics intuitively](https://www.washingtonpost.com/news/speaking-of-science/wp/2015/04/02/new-study-reveals-the-shockingly-complex-thought-processes-of-infants/?utm_term=.dd0b9545030b). 

Also, it is empirically observable that babies use that intuition to develop abstractions of knowledge, [in a reinforcement learning like manner](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621/).



Part C - Algorithms for reinforcement learning and physics
======
Now, I knew beforehand of two types of major deep learning models, that:

(1) used reinforcement learning. ([Deepmind Atari q](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf))

(2) learn laws of physics. ([Uetorch](https://github.com/facebook/UETorch))

**However**:

(a) Object detectors like [(2)](https://github.com/facebook/UETorch) use something called [pooling](http://iamaaditya.github.io/2016/03/one-by-one-convolution/) to gain translation invariance over objects, so that the model learns regardless of where the object in the image is positioned.

(b) Instead, [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  excludes pooling, because [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  requires translation variance, in order for Q learning to apply on the changing positions of the objects in pixels.


Part D - Problem discussion...
======
As a result I sought a model that could deliver both translation invariance and variance at the same time, and reasonably, **part of the solution** was models that disentangled factors of variation, i.e. [manifold learning frameworks](https://arxiv.org/abs/1611.03383).

I didn't stop my scientific thinking at [manifold learning](http://scikit-learn.org/stable/modules/manifold.html) though.

Given that cognitive science may be used to constrain machine learning models ([similar to how firms like Deepmind often use cognitive science as a boundary on the deep learning models they produce](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) I sought to create a disentanglable model that was as constrained by cognitive science, as far as algebra would permit.



Part E - Problem approach...
======
As a result I created something called the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning). (A part of a system called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)). 

This was due to [evidence of supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134); I compacted machine learning related algebra for disentangling, in the regime of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold). This could be seen as an extension of [manifold learning in artificial intelligence](http://scikit-learn.org/stable/modules/manifold.html).

**Given that the [supermanifold hypothesis](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning) compounds** ϕ(x,θ,![alt text](https://i.imgur.com/ncrjUdkm.png))<SUP>T</SUP>w, **here is an annotation of the hypothesis:**


1. [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) entails ϕ(x;θ)<SUP>T</SUP>w, that denotes the input space x, and learnt representations θ.
2. [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) underlines that coordinates or latent spaces in the [manifold](https://en.wikipedia.org/wiki/Manifold) framework, are learnt features/representations, or directions that are sparse configurations of coordinates.
3. [Supermathematics](https://en.wikipedia.org/wiki/Supermathematicsg) entails (x,θ,![alt text](https://i.imgur.com/ncrjUdkm.png)), that denotes some x valued coordinate distribution, and by extension, directions that compact coordinates via θ,![alt text](https://i.imgur.com/ncrjUdkm.png)
4.  As such, the aforesaid (x,θ,![alt text](https://i.imgur.com/ncrjUdkm.png)), is subject to coordinate transformation.
5. Thereafter 1, 2, 3, 4 and [supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134), within the generalizable nature of [euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), reasonably effectuate ϕ(x,θ,![alt text](https://i.imgur.com/ncrjUdkm.png))<SUP>T</SUP>w.


Part F - A probable experiment: A Transverse Field Ising Spin (Super)–Hamiltonian Quantum Computation 
=====
![alt text](https://i.imgur.com/L96rxC3.png)


Part G - Limitations
====

**Notably**, although [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) may turn out to be invalid in its simple description in relation to [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence), there is a non-trivial possibility that the math of [Supermanifolds](https://en.wikipedia.org/wiki/Supermanifold) may inspire future Deep Learning; [cutting edge Deep learning work](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) tends to consider boundaries in the biological brain, [and biological brains can be evaluated using supersymmetric operations](https://arxiv.org/abs/0705.1134).

In broader words, I consider the following evidence:

(1) **Manifolds** are in the regime of **very general algorithms**, where many degrees of freedom are learnable, such that, for example: [models gain the ability to posses translation in-variance and translation variance at the same time](https://arxiv.org/abs/1606.05579). (i.e. disentangling factors of variation).

(2) Given (1), and the generalizability of euclidean space, together with the instance that there persists [supersymmetric measurements in biological brains](https://arxiv.org/abs/0705.1134), it is not absurd that **Supermathematics** or **Lie Superalgebras** (in **Supermanifolds**) may eventually empirically apply in [Deep learning](https://en.wikipedia.org/wiki/Deep_learning), or some other named study of hierarchical learning in research.

Part H - Considerations
======
Notably, an ***initial degree*** of the
[(Super-) Hamiltonian](https://arxiv.org/abs/hep-th/0506170) structure required by [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) shall require a quite scalable scheme, such as some [boson sampling](https://en.wikipedia.org/wiki/Boson_sampling) aligned regime in particular, in conjunction with [supersymmetric space](https://arxiv.org/abs/0705.1134). This scheme is approachable [on the scale of 42 qubits](https://www.researchgate.net/publication/316643055_No_imminent_quantum_supremacy_by_boson_sampling#pf6), or a 42 qubit = ![alt text](https://i.imgur.com/qTHl2uA.png) gb = 131,072 gb ram configuration for simple task/circuit tests.

Separately, I am working to determine how feasible the  [model](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is.

I am working to design suitable experiments, and figuring out what type of ![alt text](https://i.imgur.com/4P5rY64.png) (training samples) are sufficiently applicable to the [model](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis).

Remember, if you have good knowledge of [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning, you may pitch in for a discussion, by messaging me at jordanmicahbennett@gmail.com.

Part I - Extras
======

I compiled a list of resources (beyond things cited throughout the papers) that may be helpful [here](https://github.com/JordanMicahBennett/Supermathematics-and-Artificial-General-Intelligence/blob/master/extra%20list%20of%20helpful%20resources.md).



Part J - Article Download
======
Here is this article in [pdf form](https://www.researchgate.net/publication/319523372_Supermathematics_and_Artificial_General_Intelligence).


