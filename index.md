![alt text](https://i.imgur.com/lQJjpAk.gif)


This thread concerns attempts to construct artificial general intelligence, which I often underline [may likely be mankind's last invention.](https://www.youtube.com/watch?v=9snY7lhJA4c)

I clearly unravel how I came to invent the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), (a component in another description called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) i.e. the &quot;**[Supersymmetric Artificial Neural Network](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)**&quot;) in relation to quantum computation.

I am asking anybody that knows [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning to pitch in the discussion below.

Part A - What is the goal?
======
1. The aim is to build [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).

2. Machine learning often concerns [constraining algorithms with respect to biological examples](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/).

3. Babies are great  examples of **some non-trivial basis** for [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence); babies are significant examples of biological baseis that are reasonably usable to inspire smart algorithms, especially in the aims of (1), regarding (2). **Babies' brains** are fantastic measures of &quot;**tabula rasa**&quot;-like states, [from which complicated abstractions are learnt into adulthood](http://www.nature.com/neuro/journal/v20/n4/box/nn.4506_BX2.html); similar to how the recent breakthrough artificial intelligence program, &quot;[AlphaGo Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero)&quot;, started out essentially &quot;**blank**&quot; beginning from random plays, up until it quickly **learnt** to become the planet's strongest go player today. ([This quick outline](https://medium.com/@jordanmicahbennett/why-did-deepmind-choose-the-alphago-game-rather-than-some-real-world-problem-55843c8ebcb9) highlights the **critical relevance** of **games** as necessary testbeds/algorithm **training** scenarios, in the aim of developing [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).)

[Thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) together with the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), espouses the **importance** of [considering biological constraints](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) in the aim of developing  [general machine learning models](https://en.wikipedia.org/wiki/Artificial_general_intelligence), pertinently, where [babies&#39; brains are observed to be pre-equipped with particular &quot;**physics priors**&quot;, constituting specifically, the ability for babies to intuitively know laws of physics, while learning by reinforcement](http://science.sciencemag.org/content/348/6230/91).

It is palpable that the phrasing **“intuitively know laws of physics”** above, should **not be confused** for nobel laureate or physics undergrad aligned babies that for example, write or understand physics papers/exams; instead, the aforesaid phrasing simply conveys that **babies' brains are pre-baked with ways to naturally exercise physics based expectations w.r.t. interactions with objects in their world**, as indicated by [Aimee Stahl and Lisa Feigenson](http://science.sciencemag.org/content/348/6230/91).

Outstandingly, the importance of **recognizing underlying causal physics laws in learning models** (although **not via supermanifolds** , as encoded in [Thought Curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)), has recently been both [demonstrated](https://arxiv.org/abs/1606.05579) and separately [echoed by Deepmind](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/), and of late, distinctly [emphasized](https://arxiv.org/abs/1709.08568) by Yoshua Bengio.




So, recalling part a above, it may be observed that babies know physics, plus they learn
======

...



Part B - Algorithms for reinforcement learning and physics
======
Now, I knew beforehand of two types of major deep learning models, that:

1. used reinforcement learning. ([Deepmind Atari q](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf))

2. learn laws of physics. ([Uetorch](https://github.com/facebook/UETorch))

**However**:

(a). Object detectors like [(2)](https://github.com/facebook/UETorch) use something called [pooling](http://iamaaditya.github.io/2016/03/one-by-one-convolution/) to gain translation invariance over objects, so that the model learns regardless of where the object in the image is positioned.

(b). Instead, [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  excludes pooling, because [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  requires translation variance, in order for Q learning to apply on the changing positions of the objects in pixels.


Part C - Problem discussion...
======
As a result I sought a model that could deliver both translation invariance and variance at the same time, and reasonably, **part of the solution** was models that disentangled factors of variation, i.e. [manifold learning frameworks](https://arxiv.org/abs/1611.03383).

I didn't stop my scientific thinking at [manifold learning](http://scikit-learn.org/stable/modules/manifold.html) though.

Given that cognitive science may be used to constrain machine learning models ([similar to how firms like Deepmind often use cognitive science as a boundary on the deep learning models they produce](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) I sought to create a disentanglable model that was as constrained by cognitive science, as far as algebra would permit.



Part D - Problem approach...
======
As a result I created something called the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning). (A part of a system called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)). 

This was due to [evidence of supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134); I compacted machine learning related algebra for disentangling, in the regime of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold). This could be seen as an extension of [manifold learning in artificial intelligence](http://scikit-learn.org/stable/modules/manifold.html).

**Given that the [supermanifold hypothesis](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning) compounds** ϕ(x,θ,θ¯)<SUP>T</SUP>w, **here is an annotation of the hypothesis:**


1. [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) entails ϕ(x,θ)<SUP>T</SUP>w, that denotes the input space x, and learnt representations ![alt text](https://i.imgur.com/PRSAGxn.png).
2. [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) underlines that coordinates or latent spaces in the [manifold](https://en.wikipedia.org/wiki/Manifold) framework, are learnt features/representations, or directions that are sparse configurations of coordinates.
3. [Supermathematics](https://en.wikipedia.org/wiki/Supermathematicsg) entails (x,θ,θ¯), that denotes some x valued coordinate distribution, and by extension, directions that compact coordinates via !θ,θ¯
4.  As such, the aforesaid (x,θ,θ¯), is subject to coordinate transformation.
5. Thereafter 1, 2, 3, 4 and [supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134), within the generalizable nature of [euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), reasonably effectuate ϕ(x,!θ,θ¯)<SUP>T</SUP>w.


Part E - A probable experiment: A Transverse Field Ising Spin (Super)–Hamiltonian Quantum Computation 
=====
![alt text](https://i.imgur.com/wgCnbTp.png)


Part F - Limitations
====

**Although** [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is minor particularly in its simple description (acquiescing [SQCD](https://arxiv.org/abs/1104.1425)) in relation to [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_General_Intelligence), it **crucially** delineates that the math of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold) is reasonably applicable in [Deep Learning](https://en.wikipedia.org/wiki/Deep_Learning), imparting that [cutting edge  Deep Learning work tends to consider boundaries in the biological brain](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) while underscoring that biological brains can be **optimally** evaluated using [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) operations.

In broader words, [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) occurs on the following evidence:

1. [Manifolds](https://en.wikipedia.org/wiki/Manifold) are in the regime of very [general algorithms](http://scikit-learn.org/stable/modules/manifold.html), that enable models to **learn many degrees of freedom** in latent space, (i.e. position, scale etc… where said degrees are observable as features of **physics** interactions) where transformations on points may represent for e.g., features of a particular object in pixel space, and transformations on said points or weights of an object are **disentangleable** or separable from those pertaining to other objects in latent space. ([Early Visual Concept Learner](https://arxiv.org/abs/1606.05579), [Mean field theory expressivity networks](https://arxiv.org/abs/1606.05340), etc)

2. Given (1), and the generalizability of  [euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), together with the instance that there persists  [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) measurements in biological brains, thought curvature predicates that  [Supermathematics](https://en.wikipedia.org/wiki/Supermathematics) or  [Lie Superalgebras](https://en.wikipedia.org/wiki/Lie_Superalgebras) (in  [Supermanifolds](https://en.wikipedia.org/wiki/Supermanifold)) may reasonably, empirically apply in  [Deep Learning](https://en.wikipedia.org/wiki/Deep_Learning), or some other named study of hierarchical learning in research.

Part G - A brief discussion on the significance of a Transverse Field Ising Spin (Super)-Hamiltonian learning algorithm
======
The usage of [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) operations is **imperatively** efficient, as such operations enable **deeply abstract representations** (as is naturally afforded by symmetry group Lie Superalgebras (See [source 1](https://arxiv.org/abs/0705.1134), [source 2](https://en.wikipedia.org/wiki/Symmetry_group#See_also))), pertinently, in a [general, biologically tenable time-space complex optimal regime](https://arxiv.org/abs/0705.1134). 

As such, said **deeply abstract representations** may **reasonably capture** certain **“physics priors”**, (See **page 4** of the [paper](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)) abound the [laws of physics](https://en.wikipedia.org/wiki/Laws_of_science).  


Part H - An informal proof of the representation power gained by deeper abstractions of the &quot;Supersymmetric Artificial Neural Network&quot;
======

Machine learning non-trivially concerns the application of families of functions that  **guarantee more and more variations**  in weight space.

This means that machine learning researchers study what functions are best to transform the weights of the artificial neural network, such that the  **weights learn** to represent  **good values** for which  **correct hypotheses or guesses** can be produced by the artificial neural network.

**The**  [**"Supersymmetric Artificial Neural Network"**](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is yet another way to  **represent richer values**  in the weights of the model; because  **supersymmetric values** can allow for **more information to be captured about the input space.** For example, supersymmetric systems can capture **potential-partner** signals, which are **beyond** the feature space of **magnitude** and **phase signals** learnt in **typical**   **real valued neural nets** and **deep complex neural networks** respectively.  As such, a brief historical progression of geometric solution spaces for varying neural network architectures follows:

![alt text](https://i.imgur.com/h23VvsS.png)


Part I - Considerations
======
Notably, an ***initial degree*** of the
[(Super-) Hamiltonian](https://arxiv.org/abs/hep-th/0506170) structure required by [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) shall require a quite scalable scheme, such as some [boson sampling](https://en.wikipedia.org/wiki/Boson_sampling) aligned range, in conjunction with [supersymmetric space](https://arxiv.org/abs/0705.1134). This scheme is approachable [on the scale of 42 qubits](https://www.researchgate.net/publication/316643055_No_imminent_quantum_supremacy_by_boson_sampling#pf6), or a 42 qubit = ![alt text](https://i.imgur.com/qTHl2uA.png) gb = 131,072 gb ram configuration for simple task/circuit tests.

Separately, I am working to determine how feasible the  [model](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is.

I am working to design suitable experiments, and figuring out what type of ![alt text](https://i.imgur.com/4P5rY64.png) (training samples) are sufficiently applicable to the [model](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis).

Remember, if you have good knowledge of [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning, you may pitch in for a discussion, by messaging me at jordanmicahbennett@gmail.com.

Part J - Extras
======

I compiled a list of resources (beyond things cited throughout the papers) that may be helpful [here](https://github.com/JordanMicahBennett/Supermathematics-and-Artificial-General-Intelligence/blob/master/1.%20Extra%20list%20of%20helpful%20resources.md).



Part K - Article Download
======
Here is this article in [pdf form](https://www.researchgate.net/publication/319523372_Supermathematics_and_Artificial_General_Intelligence).


