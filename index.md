![alt text](https://i.imgur.com/lQJjpAk.gif)


This thread concerns attempts to contribute to the field of artificial general intelligence, which I often underline [may likely be mankind's last invention.](https://www.youtube.com/watch?v=9snY7lhJA4c)

I clearly unravel how I came to invent the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), (a component in another description called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis), consisting of the &quot;**[Supersymmetric Artificial Neural Network](https://github.com/JordanMicahBennett/Supersymmetric-artificial-neural-network)**&quot;) in relation to quantum computation.

I am asking anybody that knows [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning to pitch in the discussion below.

Part A - What is the goal?
======
1. The aim is to contribute to the field of [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).

2. Machine learning often concerns [constraining algorithms with respect to biological examples](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/).

3. Babies are great  examples of **some non-trivial basis** for [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence); babies are significant examples of biological baseis that are reasonably usable to inspire smart algorithms, especially in the aims of (1), regarding (2). **Babies' brains** are fantastic measures of &quot;**tabula rasa**&quot;-like states, [from which complicated abstractions are learnt into adulthood](http://www.nature.com/neuro/journal/v20/n4/box/nn.4506_BX2.html); similar to how the recent breakthrough artificial intelligence program, &quot;[AlphaGo Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero)&quot;, started out essentially &quot;**blank**&quot; beginning from random plays, up until it quickly **learnt** to become the planet's strongest go player today. ([This quick outline](https://medium.com/@jordanmicahbennett/why-did-deepmind-choose-the-alphago-game-rather-than-some-real-world-problem-55843c8ebcb9) highlights the **critical relevance** of **games** as necessary testbeds/algorithm **training** scenarios, in the aim of developing [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).)

[Thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) together with the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), espouses the **importance** of [considering biological constraints](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) in the aim of developing  [general machine learning models](https://en.wikipedia.org/wiki/Artificial_general_intelligence), pertinently, where [babies&#39; brains are observed to be pre-equipped with particular &quot;**physics priors**&quot;, constituting specifically, the ability for babies to intuitively know laws of physics, while learning by reinforcement](http://science.sciencemag.org/content/348/6230/91).

It is palpable that the phrasing **“intuitively know laws of physics”** above, should **not be confused** for nobel laureate or physics undergrad aligned babies that for example, write or understand physics papers/exams; instead, the aforesaid phrasing simply conveys that **babies' brains are pre-baked with ways to naturally exercise physics based expectations w.r.t. interactions with objects in their world**, as indicated by [Aimee Stahl and Lisa Feigenson](http://science.sciencemag.org/content/348/6230/91).

Outstandingly, the importance of **recognizing underlying causal physics laws in learning models** (although **not via supermanifolds** , as encoded in [Thought Curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)), has recently been both [demonstrated](https://arxiv.org/abs/1606.05579) and separately [echoed by Deepmind](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/), and of late, distinctly [emphasized](https://arxiv.org/abs/1709.08568) by Yoshua Bengio.




So, recalling part a above, it may be observed that babies know physics, plus they learn
======

...



Part B - Algorithms for reinforcement learning and physics
======
Now, I knew beforehand of two types of major deep learning models, that:

1. used reinforcement learning. ([Deepmind Atari q](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf))

2. learn laws of physics. ([Uetorch](https://github.com/facebook/UETorch))

**However**:

(a). Object detectors like [(2)](https://github.com/facebook/UETorch) use something called [pooling](http://iamaaditya.github.io/2016/03/one-by-one-convolution/) to gain translation invariance over objects, so that the model learns regardless of where the object in the image is positioned.

(b). Instead, [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  excludes pooling, because [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  requires translation variance, in order for Q learning to apply on the changing positions of the objects in pixels.


Part C - Problem discussion...
======
As a result I sought a model that could deliver both translation invariance and variance at the same time, and reasonably, **part of the solution** was models that disentangled factors of variation, i.e. [manifold learning frameworks](https://arxiv.org/abs/1611.03383).

I didn't stop my scientific thinking at [manifold learning](http://scikit-learn.org/stable/modules/manifold.html) though.

Given that cognitive science may be used to constrain machine learning models ([similar to how firms like Deepmind often use cognitive science as a boundary on the deep learning models they produce](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) I sought to create a disentanglable model that was as constrained by cognitive science, as far as algebra would permit.



Part D - Problem approach...
======
As a result I created something called the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning). (A part of a system called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)). 

This was due to [evidence of supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134); I compacted machine learning related algebra for disentangling, in the regime of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold). This could be seen as an extension of [manifold learning in artificial intelligence](http://scikit-learn.org/stable/modules/manifold.html).

![alt text](https://i.imgur.com/tnIwxwE.jpg)


Part E - A probable experiment: A Transverse Field Ising Spin (Super)–Hamiltonian Quantum Computation 
=====
![alt text](https://i.imgur.com/uV0Nb7C.png)


Part F - Limitations
====

**Although** [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is minor particularly in its simple description (acquiescing [SQCD](https://arxiv.org/abs/1104.1425)) in relation to [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_General_Intelligence), it **crucially** delineates that the math of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold) is reasonably applicable in [Deep Learning](https://en.wikipedia.org/wiki/Deep_Learning), imparting that [cutting edge  Deep Learning work tends to consider boundaries in the biological brain](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) while underscoring that biological brains can be **optimally** evaluated using [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) operations.

In broader words, [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) occurs on the following evidence:

1. [Manifolds](https://en.wikipedia.org/wiki/Manifold) are in the regime of very [general algorithms](http://scikit-learn.org/stable/modules/manifold.html), that enable models to **learn many degrees of freedom** in latent space, (i.e. position, scale etc… where said degrees are observable as features of **physics** interactions) where transformations on points may represent for e.g., features of a particular object in pixel space, and transformations on said points or weights of an object are **disentangleable** or separable from those pertaining to other objects in latent space. ([Early Visual Concept Learner](https://arxiv.org/abs/1606.05579), [Mean field theory expressivity networks](https://arxiv.org/abs/1606.05340), etc)

2. Given (1), and the generalizability of  [euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), together with the instance that there persists  [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) measurements in biological brains, thought curvature predicates that  [Supermathematics](https://en.wikipedia.org/wiki/Supermathematics) or  [Lie Superalgebras](https://en.wikipedia.org/wiki/Lie_Superalgebras) (in  [Supermanifolds](https://en.wikipedia.org/wiki/Supermanifold)) may reasonably, empirically apply in  [Deep Learning](https://en.wikipedia.org/wiki/Deep_Learning), or some other named study of hierarchical learning in research.

Part G - A brief discussion on the significance of a Transverse Field Ising Spin (Super)-Hamiltonian learning algorithm
======
The usage of [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) operations is **imperatively** efficient, as such operations enable **deeply abstract representations** (as is naturally afforded by symmetry group Lie Superalgebras (See [source 1](https://arxiv.org/abs/0705.1134), [source 2](https://en.wikipedia.org/wiki/Symmetry_group#See_also))), pertinently, in a [general, biologically tenable time-space complex optimal regime](https://arxiv.org/abs/0705.1134). 

As such, said **deeply abstract representations** may **reasonably capture** certain **“physics priors”**, (See **page 4** of the [paper](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)) abound the [laws of physics](https://en.wikipedia.org/wiki/Laws_of_science).  


Part H - An informal proof of the representation power gained by deeper abstractions of the &quot;Supersymmetric Artificial Neural Network&quot;
======

Machine learning non-trivially concerns the application of families of functions that  **guarantee more and more variations**  in weight space.

This means that machine learning researchers study what functions are best to transform the weights of the artificial neural network, such that the  **weights learn** to represent  **good values** for which  **correct hypotheses or guesses** can be produced by the artificial neural network.

**The**  [**"Supersymmetric Artificial Neural Network"**](https://github.com/JordanMicahBennett/Supersymmetric-artificial-neural-network) (a core component in [‘thought curvature’](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis_-_on_the_%27Supersymmetric_Artificial_Neural_Network%27)) is yet another way to  **represent richer values**  in the weights of the model; because  **supersymmetric values** can allow for **more information to be captured about the input space.** For example, supersymmetric systems can capture **potential-partner** signals, which are **beyond** the feature space of **magnitude** and **phase signals** learnt in **typical**   **real valued neural nets** and **deep complex neural networks** respectively.  As such, a brief historical progression of geometric solution spaces for varying neural network architectures follows:

![alt text](https://i.imgur.com/NRA0CH3.png)

Part I - Naive Archictecture for the “Supersymmetric Artificial Neural Network" 
======
Following, is another view of “solution geometry” history, which may promote a clear way to view the reasoning behind the subsequent naive architecture sequence:

![alt text](https://i.imgur.com/vbon07Z.png)

I call an “Edward Witten/String theory powered artificial neural network”, ‘simply’ an artificial neural network that learns [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) weights.

Looking at the above progression of ‘solution geometries’; going from [`SO(n)`](https://en.wikipedia.org/wiki/Perceptron) representation to [`SU(n)`](https://arxiv.org/pdf/1612.05231.pdf) representation has guaranteed richer and richer representations in weight space of the artificial neural network, and hence better and better hypotheses were generatable. It is only then somewhat natural to look to [`SU(m|n)`](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis_-_on_the_%27Supersymmetric_Artificial_Neural_Network%27) representation, i.e. the **“Edward Witten/String theory powered artificial neural network” (“Supersymmetric Artificial Neural Network”)**.

To construct an “Edward Witten/String theory powered artificial neural network”, it may be feasible to compose a system, which includes a [grassmann manifold artificial neural network](https://arxiv.org/pdf/1611.05742.pdf) then [generate ‘charts’](http://www.math.wisc.edu/~robbin/761dir/grassmann.pdf) until [scenarios occur](https://en.wikipedia.org/wiki/Supersymmetry) where the “Edward Witten/String theory powered artificial neural network” is perhaps achieved, in the following way:

![alt text](https://i.imgur.com/Qz1tseV.png)

Part J - Artificial neural network/symmetry group landscape visualization
===================
![](https://i.imgur.com/KdcuSUa.png)
paper: https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis


Part K - Conclusion
======
Pertinently, the “Edward Witten/String theory powered supersymmetric artificial neural network”, is one wherein supersymmetric weights are sought.
Many machine learning algorithms are not empirically shown to be exactly biologically plausible, i.e. Deep Neural Network algorithms, have not been observed to occur in the brain, but regardless, such algorithms work in practice in machine learning. 

Likewise, regardless of Supersymmetry's elusiveness at the LHC, as seen above, it may be quite feasible to borrow formal methods from strategies in physics even if such strategies are yet to show related physical phenomena to exist; thus it may be pertinent/feasible to try to construct a model that learns supersymmetric weights, as I proposed throughout this paper, following the progression of solution geometries going from SO(n) to SU(n) and onwards to [`SU(m|n)`](https://en.wikipedia.org/wiki/Supergroup_(physics)).


Remember, if you have good knowledge of [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning, you may pitch in for a discussion, by messaging me at jordanmicahbennett@gmail.com.

Part L - Extras
======

I compiled a list of resources (beyond things cited throughout the papers) that may be helpful [here](https://github.com/JordanMicahBennett/Supermathematics-and-Artificial-General-Intelligence/blob/master/1.%20Extra%20list%20of%20helpful%20resources.md).



Part M - Article Download
======
Here is this article in [pdf form](https://www.researchgate.net/publication/319523372_Supermathematics_and_Artificial_General_Intelligence).


